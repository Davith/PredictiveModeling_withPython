{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Why the Failure?\n",
    "Recall that before the break, we showed a linear model that performed well on the training dataset but poorly on a test set.  \n",
    "\n",
    "**Review Perforance Numbers on Training Set and Test Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Overfitting in Pictures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://radimrehurek.com/data_science_python/plot_bias_variance_examples_2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='http://radimrehurek.com/data_science_python/plot_bias_variance_examples_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://upload.wikimedia.org/wikipedia/commons/1/19/Overfitting.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='http://upload.wikimedia.org/wikipedia/commons/1/19/Overfitting.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Prevent Overfitting?\n",
    "Ultimately we don't want to build a model which performs well on data we've already seen, we want to build a model which will perform well on data we haven't seen.\n",
    "\n",
    "There are two linked strategies for to accomplish this: regularization and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "###Linear Regression Loss Function\n",
    "\\begin{eqnarray*}\n",
    "    Loss(\\beta) = MSE &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat y_i)^2 \\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 \\\\   \n",
    "\\end{eqnarray*}\n",
    "\n",
    "###L2 Regularized Linear Regression Loss Function -- \"Ridge\"\n",
    "\\begin{eqnarray*}\n",
    "    Loss(\\beta) = MSE &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha ||\\beta||_2^2\\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\beta^T \\beta\\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\sum_{d=1}^D \\beta_d^2\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Ridge (alpha = .5)\n",
    "# TODO: ridge regression on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###L1 Regularized Linear Regression Loss Function -- \"LASSO\"\n",
    "\\begin{eqnarray*}\n",
    "    Loss(\\beta) = MSE &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha ||\\beta||_1\\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\sum_{d=1}^D \\beta_d\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: LASSO on dataset\n",
    "clf = linear_model.Lasso(alpha = 0.1)\n",
    "\n",
    "# TODO: interpretation, look at betas, plot betas\n",
    "# TODO: the L1 diamond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this affect our $\\beta$ vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###L1 + L2 Regularized Linear Regression Loss Function -- \"ElasticNet\"\n",
    "\\begin{eqnarray*}\n",
    "    Loss(\\beta) = MSE &=& \\frac{1}{2N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\rho ||\\beta||_1 + \\frac{\\alpha (1 - \\rho)}{2} ||\\beta||_2^2\\\\\\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\rho \\sum_{d=1}^D \\beta_d + \\frac{\\alpha (1 - \\rho)}{2} \\sum_{d=1}^D \\beta_d^2\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##How to Choose $\\alpha$ and/or $\\rho$?  Cross Validation\n",
    "There are many forms of cross validation.  The basic idea of each is to _train_ your model on some data and _estimate it's future performance_ on other data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Cross Validation\n",
    "### Validation Set Cross Validation\n",
    "1. Pick an amount of data to be in your validation set (e.g. 10%)\n",
    "2. Randomly split datapoints into training points (90%) and validation points (10%)\n",
    "3. Train your model on the training data\n",
    "4. Test your model on the validation data, record the validation error\n",
    "5. Estimated future errors is the validation error\n",
    "\n",
    "\n",
    "* **Good:** Easy and computationally cheap\n",
    "* **Bad:** Statistically noisy and wastes data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out Cross Validation\n",
    "1. For each datapoint in the training set\n",
    "  1. Train the model on all data except that datapoint\n",
    "  2. Record your error measure on the chosen datapoint\n",
    "2. Estimated future error is total error across all the datapoints \n",
    "\n",
    "\n",
    "* **Good:** Doesn't waste data\n",
    "* **Bad:** Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "1. Partition the data into K folds\n",
    "2. For each fold k:\n",
    "  1. Train the model on all your data except the data in k\n",
    "  2. Record the error on the the data in k\n",
    "3. Estimate future error as total error across all folds\n",
    "\n",
    "\n",
    "* **Good:** Computationally cheaper than leave one out cross validation, only wastes 100/k% of the data\n",
    "* **Bad:** k times as expensive as just training one model, wastes 100/k% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection with Cross Validation\n",
    "1. For each model:\n",
    "  1. Estimate the model's performance on future data using cross validation\n",
    "2. Pick the model with the best estimated future performance\n",
    "3. Train the best model from scratch on the full dataset.  This is your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: CV with data\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "clf = linear_model.LassoCV(alphas=[0.1, 1.0, 10.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats:\n",
    "* You can still overfit with intensive cross validation!\n",
    "* But it's much better than without"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "* **The Central Thesis of Machine Learning:** We're only interested in predictive performance on unseen data, NOT seen data.\n",
    "* Training set error estimates error on **seen** data\n",
    "* Cross validation error estimates error on **unseen** data\n",
    "* Regularization is a way to improve your error on unseen data, but it introduces new hyperparameters\n",
    "* Use a cross validated estimate of future performance to choose your model / hyperparameter settings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
