{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://datascience.uci.edu/wp-content/uploads/sites/2/2014/09/data_science_logo_with_image1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "Image(url='http://datascience.uci.edu/wp-content/uploads/sites/2/2014/09/data_science_logo_with_image1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, let's define a few helper functions to use later.  \n",
    "# Don't worry about trying to pick these apart yet\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate the mean_squared_error given a vector of true ys and a vector of predicted ys\n",
    "    \"\"\"\n",
    "    diff = y_true - y_pred\n",
    "    return np.sqrt(np.dot(diff, diff)) / len(diff)\n",
    "\n",
    "def calc_train_test_errors(untrained_model, x_train, x_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    calculate the training and testing l2 error of a model\n",
    "    \"\"\"\n",
    "    model = untrained_model\n",
    "    model.fit(x_train, y_train)\n",
    "    y_predicted_train = model.predict(x_train)\n",
    "    y_predicted_test = model.predict(x_test)\n",
    "\n",
    "    train_error = mean_squared_error(y_train, y_predicted_train)\n",
    "    test_error = mean_squared_error(y_test, y_predicted_test)\n",
    "\n",
    "    model_name = get_model_name(model)\n",
    "    \n",
    "    errors = pd.DataFrame(data=[[model_name, train_error, test_error]], \n",
    "                          columns=['Model Name', 'Train Error', 'Test Error'])\n",
    "    \n",
    "    return errors\n",
    "\n",
    "def calc_true_and_predicted_ys(trained_model, x, y_true):\n",
    "    y_predicted = trained_model.predict(x)    \n",
    "\n",
    "    diff = y_true - y_predicted\n",
    "    diff_squared = diff * diff\n",
    "    \n",
    "    ys_df = pd.DataFrame(data=np.vstack([y_true, y_predicted, diff_squared]).T, \n",
    "                         columns=['True y', 'Predicted y', 'Squared Error'])\n",
    "    \n",
    "    return ys_df    \n",
    "\n",
    "def get_model_name(model):\n",
    "    s = model.__str__().lower()\n",
    "    if \"linearregression\" in s:\n",
    "        return 'LinearRegression'\n",
    "    elif \"lasso\" in s:\n",
    "        return 'Lasso(a=%g)' % model.alpha\n",
    "    elif \"ridge\" in s:\n",
    "        return 'Ridge(a=%g)' % model.alpha\n",
    "    elif \"elastic\" in s:\n",
    "        return 'ElasticNet(a=%g, r=%g)' % (model.alpha, model.l1_ratio)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Model Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review From Before Lunch:\n",
    "We created a linear model and saw that it performed well on half the data but poorly on the other half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted L2 Error: 0.0453157064018\n",
      "\n",
      "Predicted L2 Error: 0.0604328385826\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "with np.load('data/mystery_data_old.npz') as data:\n",
    "    celeb_data_old = data['celeb_data_old']\n",
    "    popularity_old = data['popularity_old']\n",
    "    celeb_data_new = data['celeb_data_new']\n",
    "\n",
    "lmr3 = linear_model.LinearRegression()\n",
    "lmr3.fit(celeb_data_old, popularity_old)\n",
    "predicted_popularity_old = lmr3.predict(celeb_data_old)\n",
    "predicted_popularity_new = lmr3.predict(celeb_data_new)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate the mean_squared_error given a vector of true ys and a vector of predicted ys\n",
    "    \"\"\"\n",
    "    diff = y_true - y_pred\n",
    "    return np.sqrt(np.dot(diff, diff)) / len(y_true)\n",
    "\n",
    "print \"Predicted L2 Error:\", mean_squared_error(popularity_old, predicted_popularity_old)\n",
    "print \n",
    "\n",
    "with np.load('data/mystery_data_new.npz') as data:\n",
    "    popularity_new = data['popularity_new']\n",
    "\n",
    "print \"Predicted L2 Error:\", mean_squared_error(popularity_new, predicted_popularity_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Why the Failure?  We're Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting in Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://radimrehurek.com/data_science_python/plot_bias_variance_examples_2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='http://radimrehurek.com/data_science_python/plot_bias_variance_examples_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://upload.wikimedia.org/wikipedia/commons/1/19/Overfitting.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='http://upload.wikimedia.org/wikipedia/commons/1/19/Overfitting.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Prevent Overfitting?\n",
    "Ultimately we don't want to build a model which performs well on data we've already seen, we want to build a model which will perform well on data we haven't seen.\n",
    "\n",
    "There are two linked strategies for to accomplish this: regularization and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "###Linear Regression Loss Function\n",
    "\\begin{eqnarray*}\n",
    "    Loss(\\beta) = MSE &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat y_i)^2 \\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 \\\\   \n",
    "\\end{eqnarray*}\n",
    "\n",
    "###L2 Regularized Linear Regression Loss Function -- \"Ridge\"\n",
    "\\begin{eqnarray*}\n",
    "    Loss(\\beta) = MSE &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha ||\\beta||_2^2\\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\beta^T \\beta\\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\sum_{d=1}^D \\beta_d^2\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Ridge (alpha = .5)\n",
    "# TODO: ridge regression on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###L1 Regularized Linear Regression Loss Function -- \"LASSO\"\n",
    "\\begin{eqnarray*}\n",
    "    Loss(\\beta) = MSE &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha ||\\beta||_1\\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\sum_{d=1}^D \\beta_d\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: LASSO on dataset\n",
    "clf = linear_model.Lasso(alpha = 0.1)\n",
    "\n",
    "# TODO: interpretation, look at betas, plot betas\n",
    "# TODO: the L1 diamond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this affect our $\\beta$ vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###L1 + L2 Regularized Linear Regression Loss Function -- \"ElasticNet\"\n",
    "\\begin{eqnarray*}\n",
    "    Loss(\\beta) = MSE &=& \\frac{1}{2N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\rho ||\\beta||_1 + \\frac{\\alpha (1 - \\rho)}{2} ||\\beta||_2^2\\\\\\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\rho \\sum_{d=1}^D \\beta_d + \\frac{\\alpha (1 - \\rho)}{2} \\sum_{d=1}^D \\beta_d^2\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##How to Choose $\\alpha$ and/or $\\rho$?  Cross Validation\n",
    "There are many forms of cross validation.  The basic idea of each is to _train_ your model on some data and _estimate it's future performance_ on other data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Cross Validation\n",
    "### Validation Set Cross Validation\n",
    "1. Pick an amount of data to be in your validation set (e.g. 10%)\n",
    "2. Randomly split datapoints into training points (90%) and validation points (10%)\n",
    "3. Train your model on the training data\n",
    "4. Test your model on the validation data, record the validation error\n",
    "5. Estimated future errors is the validation error\n",
    "\n",
    "\n",
    "* **Good:** Easy and computationally cheap\n",
    "* **Bad:** Statistically noisy and wastes data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "1. Partition the data into K folds\n",
    "2. For each fold k:\n",
    "  1. Train the model on all your data except the data in k\n",
    "  2. Record the error on the the data in k\n",
    "3. Estimate future error as total error across all folds\n",
    "\n",
    "\n",
    "* **Good:** Computationally cheaper than leave one out cross validation, only wastes 100/k% of the data\n",
    "* **Bad:** k times as expensive as just training one model, wastes 100/k% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b2dd66a2e903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "Image(url='https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection with Cross Validation\n",
    "1. For each model:\n",
    "  1. Estimate the model's performance on future data using cross validation\n",
    "2. Pick the model with the best estimated future performance\n",
    "3. Train the best model from scratch on the full dataset.  This is your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: CV with data\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "clf = linear_model.LassoCV(alphas=[0.1, 1.0, 10.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats:\n",
    "* You can still overfit with intensive cross validation!\n",
    "* But it's much better than without"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "* **The Central Thesis of Machine Learning:** We're only interested in predictive performance on unseen data, NOT seen data.\n",
    "* Training set error estimates error on **seen** data\n",
    "* Cross validation error estimates error on **unseen** data\n",
    "* Regularization is a way to improve your error on unseen data, but it introduces new hyperparameters\n",
    "* Use a cross validated estimate of future performance to choose your model / hyperparameter settings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
